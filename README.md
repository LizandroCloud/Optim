## Optimization Algorithms

### 1. One-Dimensional Optimization without Derivatives

#### 1.1 [Dichotomous Search](https://colab.research.google.com/drive/1mqzlcm93uwzBbMJpFpLxYX3amBKHN-kR#scrollTo=VqL2zUGphW7m)
#### 1.2 Fibonacci search
#### 1.3 Golden-section
#### 1.4 Quadratic interpolation
#### 1.5 Cubic interpolation
#### 1.6 The Davies, Swann, and Campey method

### 2. One-Dimensional Gradient Methods

#### 2.1 Steepest Descent
#### 2.2 Conjugate Gradient
#### 2.3 Powell
#### 2.4 Newton
#### 2.5 Quasi-Newton 
#### 2.6 Methods used for Machine Learning (Adan, Adagrad)

### 3. Multi-Dimensional Gradient Methods

#### 3.1 Steepest Descent
#### 3.2 Conjugate Gradient
#### 3.3 Powell
#### 3.4 Newton
#### 3.5 Quasi-Newton 
#### 3.6 Methods used for Machine Learning (Adan, Adagrad)
#### 3.7 Correcting Hessian Matrix

### 4. Incorporating Constraints with Penalty Functions
